{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "Management of hyperglycemia in hospitalized patients has a significant bearing on outcome, in terms of both morbidity and mortality. However, there are few national assessments of diabetes care during hospitalization which could serve as a baseline for change. This analysis of a large clinical database was undertaken to provide such an assessment and to find future directions which might lead to improvements in patient safety.  \n",
    "  \n",
    "Based on the given data, build a model to predict whether the patient is suffering from Diabetes or not? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,classification_report,confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset_diabetes_diabetic_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping unwanted variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop([\"patient_nbr\"],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diabatic is the target variable while all others are the predictors.  \n",
    "  \n",
    "Out of the 19 columns, 8 are object type, while remaining 11 are int.  \n",
    "<b>Object</b> - race, gender, age, diag_1, diag_2, diag_3, change, diabatic  \n",
    "<b>Int</b> -admission_type_id, discharge_disposition_id, admission_source_id, time_in_hospital, num_lab_procedures, num_procedures, num_medications, number_outpatient, number_emergency, number_inpatient, number_diagnoses\n",
    "  \n",
    "Since label encoding is already present in the data, some of the nominal variables are displayed as int.  \n",
    "Actual Nominal variables are 12:\n",
    "race, gender, age, admission_type_id, discharge_disposition_id, admission_source_id, diag_1, diag_2, diag_3, number_diagnoses, change, diabatic.\n",
    "  \n",
    "Actual Numeric variables are 7:\n",
    "time_in_hospital, num_lab_procedures, num_procedures, num_medications, number_outpatient, number_emergency, number_inpatient\n",
    "\n",
    "  \n",
    "It appears there are also no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing value in any column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any missing values ?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variables has invalid character \"?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geting unique counts of all Nominal Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df[['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', \n",
    "                   'diag_1', 'diag_2', 'diag_3', 'number_diagnoses', 'change', 'diabatic']]:\n",
    "    print(column.upper(),': ',df[column].nunique())\n",
    "    print(df[column].value_counts().sort_values())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "race, diag1, diag2 and diag3 has ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diag1, diag2, diag3 â€” are categorical and have a lot of values. We will remove these and use number_diagnoses to capture some of this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before Removing: ',df.shape)\n",
    "df=df.drop([\"diag_1\",\"diag_2\",\"diag_3\"],axis=1) \n",
    "print('After Removing: ',df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with 1461 values in race variable that has ?  \n",
    "Since this is a categorical variable, we can either have these as a separate group, or we will remove them, since we still have sufficient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before: ',df.shape)\n",
    "# Replace ? to Nan and remove all missing values\n",
    "\n",
    "\n",
    "\n",
    "print('After: ',df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any duplicates ?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = df.duplicated()\n",
    "print('Number of duplicate rows = %d' % (dups.sum()))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct box plot for continuous variables\n",
    "plt.figure(figsize=(15,15))\n",
    "df[['time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient']].boxplot(vert=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are outliers in all the variables. Random Forest and ANN can handle the outliers. \n",
    "Hence, Outliers are not treated for now, we will keep the data as it is. However students are encouraged to do the outliers treatment and compare the model performances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking pairwise distribution of the continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct heatmap with only continuous variables\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(df[['time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient']].corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are mostly positive correlations between variables, and very few negative correlations.  \n",
    "Overall the magnitude of correlations between the variables are very less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting all objects to categorical codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the if statement and convert to categorical codes\n",
    "for feature in df.columns: \n",
    "    if df[feature].dtype == 'object': \n",
    "        print('\\n')\n",
    "        print('feature:',feature)\n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportion of 1s and 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no issue of class imbalance here as we have reasonable proportions in both the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the target column into separate vectors for training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"diabatic\", axis=1)\n",
    "\n",
    "y = df.pop(\"diabatic\")\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, train_labels, test_labels = train_test_split(X, y, test_size=.30, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the dimensions of the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train',X_train.shape)\n",
    "print('X_test',X_test.shape)\n",
    "print('train_labels',train_labels.shape)\n",
    "print('test_labels',test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': [10,20,30,50],\n",
    "    'min_samples_leaf': [50,100,150], \n",
    "    'min_samples_split': [150,300,450],\n",
    "}\n",
    "\n",
    "dtcl = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "grid_search = GridSearchCV(estimator = dtcl, param_grid = param_grid, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, train_labels)\n",
    "print(grid_search.best_params_)\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char_label = ['no', 'yes']\n",
    "tree_regularized = open('tree_regularized.dot','w')\n",
    "dot_data = tree.export_graphviz(best_grid, out_file= tree_regularized , feature_names = list(X_train), class_names = list(train_char_label))\n",
    "\n",
    "tree_regularized.close()\n",
    "dot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://webgraphviz.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pd.DataFrame(best_grid.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values('Imp',ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on Training and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict both the train and test data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Predicted Classes and Probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_predict\n",
    "ytest_predict_prob=best_grid.predict_proba(X_test)\n",
    "ytest_predict_prob\n",
    "pd.DataFrame(ytest_predict_prob).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC and ROC for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "probs = best_grid.predict_proba(X_train)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "# calculate AUC\n",
    "cart_train_auc = roc_auc_score(train_labels, probs)\n",
    "print('AUC: %.3f' % cart_train_auc)\n",
    "# calculate roc curve\n",
    "cart_train_fpr, cart_train_tpr, cart_train_thresholds = roc_curve(train_labels, probs)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "plt.plot(cart_train_fpr, cart_train_tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC and ROC for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "probs = best_grid.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "# calculate AUC\n",
    "cart_test_auc = roc_auc_score(test_labels, probs)\n",
    "print('AUC: %.3f' % cart_test_auc)\n",
    "# calculate roc curve\n",
    "cart_test_fpr, cart_test_tpr, cart_testthresholds = roc_curve(test_labels, probs)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "plt.plot(cart_test_fpr, cart_test_tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the confusion matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Data Accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train_labels, ytrain_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_metrics=classification_report(train_labels, ytrain_predict,output_dict=True)\n",
    "df=pd.DataFrame(cart_metrics).transpose()\n",
    "cart_train_f1=round(df.loc[\"1\"][2],2)\n",
    "cart_train_recall=round(df.loc[\"1\"][1],2)\n",
    "cart_train_precision=round(df.loc[\"1\"][0],2)\n",
    "print ('cart_train_precision ',cart_train_precision)\n",
    "print ('cart_train_recall ',cart_train_recall)\n",
    "print ('cart_train_f1 ',cart_train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_labels, ytest_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data Accuracy\n",
    "cart_test_acc=best_grid.score(X_test,test_labels)\n",
    "cart_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, ytest_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_metrics=classification_report(test_labels, ytest_predict,output_dict=True)\n",
    "df=pd.DataFrame(cart_metrics).transpose()\n",
    "cart_test_precision=round(df.loc[\"1\"][0],2)\n",
    "cart_test_recall=round(df.loc[\"1\"][1],2)\n",
    "cart_test_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('cart_test_precision ',cart_test_precision)\n",
    "print ('cart_test_recall ',cart_test_recall)\n",
    "print ('cart_test_f1 ',cart_test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the model performance and give the interpretations\n",
    "  \n",
    "<b>Train Data:</b>  \n",
    "    AUC:  %        \n",
    "    Accuracy:  %        \n",
    "    Sensitivity:  %     \n",
    "    Precision: %        \n",
    "    f1-Score: %       \n",
    "            \n",
    "<b>Test Data:</b>      \n",
    "    AUC: %      \n",
    "    Accuracy: %      \n",
    "    Sensitivity: %    \n",
    "    Precision: %       \n",
    "    f1-Score: %     \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for finding out the optimal values for the hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to large volume of data, trying for different parameter values in the gridsearch with higher cv value will have higher execution time, so the best values that came after the search are directly put in Param_grid. Students can try on there own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [10],## 20,30,40\n",
    "    'max_features': [6],## 7,8,9\n",
    "    'min_samples_leaf': [10],## 50,100\n",
    "    'min_samples_split': [50], ## 60,70\n",
    "    'n_estimators': [300] ## 100,200\n",
    "}\n",
    "\n",
    "rfcl = RandomForestClassifier(random_state=1)\n",
    "\n",
    "grid_search = GridSearchCV(estimator = rfcl, param_grid = param_grid, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Training and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_predict = best_grid.predict(X_train)\n",
    "ytest_predict = best_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Model Performance Evaluation on Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(train_labels,ytrain_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_acc=best_grid.score(X_train,train_labels) \n",
    "rf_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train_labels,ytrain_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_metrics=classification_report(train_labels, ytrain_predict,output_dict=True)\n",
    "df=pd.DataFrame(rf_metrics).transpose()\n",
    "rf_train_precision=round(df.loc[\"1\"][0],2)\n",
    "rf_train_recall=round(df.loc[\"1\"][1],2)\n",
    "rf_train_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('rf_train_precision ',rf_train_precision)\n",
    "print ('rf_train_recall ',rf_train_recall)\n",
    "print ('rf_train_f1 ',rf_train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_fpr, rf_train_tpr,_=roc_curve(train_labels,best_grid.predict_proba(X_train)[:,1])\n",
    "plt.plot(rf_train_fpr,rf_train_tpr,color='green')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "rf_train_auc=roc_auc_score(train_labels,best_grid.predict_proba(X_train)[:,1])\n",
    "print('Area under Curve is', rf_train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Model Performance Evaluation on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_labels,ytest_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_acc=best_grid.score(X_test,test_labels)\n",
    "rf_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels,ytest_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_metrics=classification_report(test_labels, ytest_predict,output_dict=True)\n",
    "df=pd.DataFrame(rf_metrics).transpose()\n",
    "rf_test_precision=round(df.loc[\"1\"][0],2)\n",
    "rf_test_recall=round(df.loc[\"1\"][1],2)\n",
    "rf_test_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('rf_test_precision ',rf_test_precision)\n",
    "print ('rf_test_recall ',rf_test_recall)\n",
    "print ('rf_test_f1 ',rf_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_fpr, rf_test_tpr,_=roc_curve(test_labels,best_grid.predict_proba(X_test)[:,1])\n",
    "plt.plot(rf_test_fpr,rf_test_tpr,color='green')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "rf_test_auc=roc_auc_score(test_labels,best_grid.predict_proba(X_test)[:,1])\n",
    "print('Area under Curve is', rf_test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Importance\n",
    "print (pd.DataFrame(best_grid.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values('Imp',ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the model performance and give the interpretations\n",
    "  \n",
    "<b>Train Data:</b>  \n",
    "    AUC:  %        \n",
    "    Accuracy:  %        \n",
    "    Sensitivity:  %     \n",
    "    Precision: %        \n",
    "    f1-Score: %       \n",
    "            \n",
    "<b>Test Data:</b>      \n",
    "    AUC: %      \n",
    "    Accuracy: %      \n",
    "    Sensitivity: %    \n",
    "    Precision: %       \n",
    "    f1-Score: %     \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'hidden_layer_sizes': [100], # 50, 200\n",
    "    'max_iter': [2500], #5000,2500\n",
    "    'solver': ['adam'], #sgd\n",
    "    'tol': [0.01], \n",
    "}\n",
    "\n",
    "nncl = MLPClassifier(random_state=1)\n",
    "\n",
    "grid_search = GridSearchCV(estimator = nncl, param_grid = param_grid, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, train_labels)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "best_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Training and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_predict = best_grid.predict(X_train)\n",
    "ytest_predict = best_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model Performance Evaluation on Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(train_labels,ytrain_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train_acc=best_grid.score(X_train,train_labels) \n",
    "nn_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train_labels,ytrain_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_metrics=classification_report(train_labels, ytrain_predict,output_dict=True)\n",
    "df=pd.DataFrame(nn_metrics).transpose()\n",
    "nn_train_precision=round(df.loc[\"1\"][0],2)\n",
    "nn_train_recall=round(df.loc[\"1\"][1],2)\n",
    "nn_train_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('nn_train_precision ',nn_train_precision)\n",
    "print ('nn_train_recall ',nn_train_recall)\n",
    "print ('nn_train_f1 ',nn_train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train_fpr, nn_train_tpr,_=roc_curve(train_labels,best_grid.predict_proba(X_train)[:,1])\n",
    "plt.plot(nn_train_fpr,nn_train_tpr,color='black')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "nn_train_auc=roc_auc_score(train_labels,best_grid.predict_proba(X_train)[:,1])\n",
    "print('Area under Curve is', nn_train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model Performance Evaluation on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_labels,ytest_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_test_acc=best_grid.score(X_test,test_labels)\n",
    "nn_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels,ytest_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_metrics=classification_report(test_labels, ytest_predict,output_dict=True)\n",
    "df=pd.DataFrame(nn_metrics).transpose()\n",
    "nn_test_precision=round(df.loc[\"1\"][0],2)\n",
    "nn_test_recall=round(df.loc[\"1\"][1],2)\n",
    "nn_test_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('nn_test_precision ',nn_test_precision)\n",
    "print ('nn_test_recall ',nn_test_recall)\n",
    "print ('nn_test_f1 ',nn_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_test_fpr, nn_test_tpr,_=roc_curve(test_labels,best_grid.predict_proba(X_test)[:,1])\n",
    "plt.plot(nn_test_fpr,nn_test_tpr,color='black')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "nn_test_auc=roc_auc_score(test_labels,best_grid.predict_proba(X_test)[:,1])\n",
    "print('Area under Curve is', nn_test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the model performance and give the interpretations\n",
    "  \n",
    "<b>Train Data:</b>  \n",
    "    AUC:  %        \n",
    "    Accuracy:  %        \n",
    "    Sensitivity:  %     \n",
    "    Precision: %        \n",
    "    f1-Score: %       \n",
    "            \n",
    "<b>Test Data:</b>      \n",
    "    AUC: %      \n",
    "    Accuracy: %      \n",
    "    Sensitivity: %    \n",
    "    Precision: %       \n",
    "    f1-Score: %     \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the performance metrics from the 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=['Accuracy', 'AUC', 'Recall','Precision','F1 Score']\n",
    "data = pd.DataFrame({'CART Train':[cart_train_acc,cart_train_auc,cart_train_recall,cart_train_precision,cart_train_f1],\n",
    "        'CART Test':[cart_test_acc,cart_test_auc,cart_test_recall,cart_test_precision,cart_test_f1],\n",
    "       'Random Forest Train':[rf_train_acc,rf_train_auc,rf_train_recall,rf_train_precision,rf_train_f1],\n",
    "        'Random Forest Test':[rf_test_acc,rf_test_auc,rf_test_recall,rf_test_precision,rf_test_f1],\n",
    "       'Neural Network Train':[nn_train_acc,nn_train_auc,nn_train_recall,nn_train_precision,nn_train_f1],\n",
    "        'Neural Network Test':[nn_test_acc,nn_test_auc,nn_test_recall,nn_test_precision,nn_test_f1]},index=index)\n",
    "round(data,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve for the 3 models on the Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(cart_train_fpr, cart_train_tpr,color='red',label=\"CART\")\n",
    "plt.plot(rf_train_fpr,rf_train_tpr,color='green',label=\"RF\")\n",
    "plt.plot(nn_train_fpr,nn_train_tpr,color='black',label=\"NN\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve for the 3 models on the Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(cart_test_fpr, cart_test_tpr,color='red',label=\"CART\")\n",
    "plt.plot(rf_test_fpr,rf_test_tpr,color='green',label=\"RF\")\n",
    "plt.plot(nn_test_fpr,nn_test_tpr,color='black',label=\"NN\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
